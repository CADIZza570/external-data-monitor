# ============================================================
# üöÄ API DATA FETCHER ‚Äì PROYECTO L√çNEA BASE (MES 1‚Äì2)
# ============================================================
# Este script cumple m√∫ltiples prop√≥sitos:
#
# 1Ô∏è‚É£ Obtener datos desde una API p√∫blica
# 2Ô∏è‚É£ Guardar datos crudos en CSV y JSON
# 3Ô∏è‚É£ Permitir pruebas con CSV locales (sin usar API)
# 4Ô∏è‚É£ Limpiar datos (duplicados, columnas nuevas)
# 5Ô∏è‚É£ Generar alertas VISIBLES en consola
# 6Ô∏è‚É£ Dejar logs para auditor√≠a futura
#
# IMPORTANTE:
# - Este archivo es intencionalmente largo (~200 l√≠neas)
# - Est√° dise√±ado para ESTUDIAR, no para compactar
# ============================================================


# =========================
# üì¶ IMPORTS
# =========================

import requests
import pandas as pd
import logging
import os
import json
import sys
from datetime import datetime
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from requests.exceptions import RequestException


# =========================
# ‚öôÔ∏è CONFIGURACI√ìN GLOBAL
# =========================

API_URL = "https://jsonplaceholder.typicode.com/users"
OUTPUT_DIR = "output"
LOG_FILE = "api_data_fetcher.log"

os.makedirs(OUTPUT_DIR, exist_ok=True)

logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)


# =========================
# üåê SESI√ìN HTTP ROBUSTA
# =========================

def create_session():
    """
    Crea una sesi√≥n HTTP con reintentos autom√°ticos.
    Esto evita que el script falle por errores temporales.
    """
    session = requests.Session()

    retries = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[500, 502, 503, 504]
    )

    adapter = HTTPAdapter(max_retries=retries)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    return session


# =========================
# üì• FETCH DESDE API
# =========================

def fetch_data_api():
    """
    Obtiene datos desde la API remota.
    Retorna un DataFrame de pandas.
    """
    print("üåê Conectando a la API...")
    session = create_session()

    try:
        response = session.get(API_URL, timeout=10)
        response.raise_for_status()
        data = response.json()

        print(f"‚úÖ Datos obtenidos: {len(data)} registros")
        logging.info("Datos obtenidos desde API correctamente")

        return pd.DataFrame(data)

    except RequestException as e:
        logging.error(f"Error al conectar con API: {e}")
        raise RuntimeError("‚ùå Error al obtener datos desde la API")


# =========================
# üìÇ FETCH DESDE CSV LOCAL
# =========================

def fetch_data_local(file_path):
    """
    Carga un CSV local para pruebas manuales.
    Ideal para probar duplicados y validaciones.
    """
    print(f"üìÇ Usando CSV local: {file_path}")

    if not os.path.exists(file_path):
        raise FileNotFoundError(f"‚ùå Archivo no encontrado: {file_path}")

    df = pd.read_csv(file_path)
    print(f"‚úÖ Archivo cargado desde {file_path} ({len(df)} filas)")
    logging.info(f"CSV local cargado: {file_path}")

    return df


# =========================
# üîç VALIDACI√ìN DE DATOS
# =========================

def validate_data(df):
    """
    Verifica que el DataFrame tenga estructura m√≠nima v√°lida.
    """
    print("üîç Validando estructura de datos...")

    required_columns = {"id", "name", "email"}

    if not required_columns.issubset(df.columns):
        logging.error("Estructura inv√°lida de datos")
        raise ValueError("‚ùå Faltan columnas obligatorias")

    print("‚úÖ Validaci√≥n exitosa")
    logging.info("Datos validados correctamente")


# =========================
# üíæ GUARDADO DE ARCHIVOS
# =========================

def save_raw_outputs(df, timestamp):
    """
    Guarda CSV y JSON originales.
    """
    csv_path = f"{OUTPUT_DIR}/users_data_{timestamp}.csv"
    json_path = f"{OUTPUT_DIR}/users_data_{timestamp}.json"

    df.to_csv(csv_path, index=False)
    df.to_json(json_path, orient="records", indent=2)

    print(f"üíæ CSV guardado: {csv_path}")
    print(f"üíæ JSON guardado: {json_path}")

    logging.info("Archivos crudos guardados")

    return csv_path


# =========================
# üìä PROCESAMIENTO DE DATOS
# =========================

def process_data(df):
    """
    Limpia datos:
    - Detecta duplicados
    - Elimina duplicados
    - Agrega columnas nuevas
    """
    print("\nüìä Iniciando procesamiento de datos...")

    original_count = len(df)

    # Detectar duplicados
    duplicated_rows = df[df.duplicated()]
    duplicated_count = len(duplicated_rows)

    if duplicated_count > 0:
        print("üö® ALERTA: SE DETECTARON DUPLICADOS")
        print(duplicated_rows)
        logging.warning(f"{duplicated_count} duplicados detectados")
    else:
        print("‚ÑπÔ∏è No se detectaron duplicados")

    # Eliminar duplicados
    df_clean = df.drop_duplicates()
    clean_count = len(df_clean)

    # Nueva columna ejemplo
    if "address" in df_clean.columns:
        df_clean["city"] = df_clean["address"].apply(
            lambda x: x.get("city") if isinstance(x, dict) else None
        )

    print(f"Registros originales: {original_count}")
    print(f"Registros limpios: {clean_count}")
    print(f"Duplicados eliminados: {original_count - clean_count}")

    return df_clean


# =========================
# üíæ GUARDADO LIMPIO
# =========================

def save_clean_csv(df, timestamp):
    """
    Guarda el CSV limpio final.
    """
    clean_path = f"{OUTPUT_DIR}/users_data_{timestamp}_clean.csv"
    df.to_csv(clean_path, index=False)

    print(f"‚úÖ CSV limpio guardado: {clean_path}")
    logging.info("CSV limpio guardado")

    return clean_path


# =========================
# üß† FUNCI√ìN PRINCIPAL
# =========================

def main():
    """
    Orquesta todo el pipeline.
    Permite:
    - python api_data_fetcher.py
    - python api_data_fetcher.py output/mi_csv_modificado.csv
    """

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # --- MODO CSV LOCAL ---
    if len(sys.argv) > 1:
        df = fetch_data_local(sys.argv[1])

    # --- MODO API ---
    else:
        df = fetch_data_api()

    validate_data(df)

    # Guardar crudo solo si viene de API
    if len(sys.argv) == 1:
        save_raw_outputs(df, timestamp)

    df_clean = process_data(df)
    save_clean_csv(df_clean, timestamp)

    print("\nüéâ Pipeline finalizado exitosamente")
    print("=" * 60)


# =========================
# ‚ñ∂Ô∏è ENTRY POINT
# =========================

if __name__ == "__main__":
    main()
