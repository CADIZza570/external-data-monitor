# NOTES.md - Registro del Proyecto L√≠nea Base

## Weekly Post-Mortem - 16 Diciembre 2025

**¬øQu√© se rompi√≥ esta semana?**  
- La API original (fake-store-api) dio error 404 y fall√≥ la primera ejecuci√≥n.

**¬øPor qu√©?**  
- APIs gratuitas hospedadas en Render pueden apagarse o cambiar de URL sin aviso.

**¬øC√≥mo lo evitar√≠a la pr√≥xima?**  
- Preferir APIs estables y conocidas como JSONPlaceholder para desarrollo.  
- En el futuro (Mes 4), documentar siempre un "Plan B" con API alternativa.

**¬øC√≥mo encaja esta semana en el plan completo?**  
- ¬°Mes 1 casi completado en un solo d√≠a!  
- Script modular con logging, validaci√≥n, guardado en output/ y manejo de errores funcionando al 100%.  
- Primera prueba real de resiliencia: detect√≥ el fallo, lo logue√≥ y seguimos adelante.  
- Proyecto L√≠nea Base listo para evolucionar en los pr√≥ximos meses.

**Pr√≥ximos pasos inmediatos:**  
- Crear README.md profesional  
- Subir todo a GitHub p√∫blico  
- Artefacto visible mensual (Capturas de consola y carpeta output/ con m√∫ltiples ejecuciones)

## Decision Log ‚Äì Data Cleaning

Chose to keep all raw outputs in /output as execution evidence.
Introduced *_clean.csv as the canonical dataset for downstream systems.

## üéâ Milestone: Repositorio limpio y .gitignore funcional - 18 Dic 2025 (tarde)

**Logros:**
- ‚úÖ .gitignore creado y configurado
- ‚úÖ 18 archivos generados removidos de Git
- ‚úÖ Push exitoso sin outputs ni logs
- ‚úÖ Repositorio profesional y mantenible

**Archivos removidos:**
- Logs: api_data_fetcher.log
- Outputs: 15+ CSVs/JSONs
- Sistema: .DS_Store

**Pr√≥ximo sprint:**
- Validaci√≥n de nicho (Columbus)
- Completar Mes 2 (read_excel, merge)
- Preparar para Mes 3 

# Exploraci√≥n de Nicho - 19 Diciembre 2025

## E-commerce (Shopify / Inventory Automation)
- Job 1 (Upwork): "Senior Analytics Engineer ‚Äì Shopify Inventory Forecasting" ‚Äì Necesitan sistema autom√°tico para forecast demand, low-stock alerts, transfers entre locations. Stack: Shopify API + Python + BigQuery. Presupuesto impl√≠cito alto (proyecto producci√≥n).
- Job 2 (Upwork/LinkedIn): M√∫ltiples para "Shopify Developer" ‚Äì Custom apps, API integration para inventory sync, dropshipping automation, stock alerts.
- Job 3 (LinkedIn): +250 jobs Shopify Developer en USA ‚Äì Temas recurrentes: custom themes, apps privadas, automation con React/Node/Python.
- Dolor com√∫n: Gesti√≥n manual de stock (low-stock, dead stock, transfers), p√©rdida de ventas por stockouts.
- Demanda: ALTA (decenas de jobs activos, presupuestos $200-400+).

## Inmobiliarias (Real Estate Leads / WhatsApp Automation)
- Jobs encontrados: Pocos espec√≠ficos para automation. Algunos generales para CRM/property management, pero no alertas WhatsApp o leads autom√°ticos recientes.
- Dolor com√∫n: Leads manuales de portales, seguimiento lento.
- Demanda: BAJA en b√∫squedas actuales (menos evidencia directa).

## Coaches / Consultores (Calendar / Onboarding Automation)
- Jobs encontrados: Casi nulos espec√≠ficos. Algunos para virtual assistants o CRM general, pero no automation de calendarios/onboarding para coaches.
- Dolor com√∫n: Gesti√≥n manual de citas y clientes nuevos.
- Demanda: BAJA (poca evidencia en plataformas freelance).

## Nicho tentativo elegido: E-commerce (Shopify Inventory & Alerts)
Razones:
- M√°s jobs reales y activos.
- Encaja perfecto con tu Proyecto L√≠nea Base (API data fetch, limpieza, alertas futuras).
- Presupuestos visibles y demanda creciente (retail multi-location necesita automation).
- F√°cil evoluci√≥n: Tu script ya maneja datos ‚Üí agregar alertas stock bajo, forecast simple con Pandas.

Pr√≥ximo: Evolucionar script para "low-stock alert" demo (Mes 3-4).

¬°Sistemas vivos en acci√≥n! üî•

# Weekly Post-Mortem - 19 Diciembre 2025 (Cierre Mes 2 / Inicio Mes 3)

¬øQu√© se rompi√≥ esta semana?
- Inicialmente plane√°bamos n8n como cerebro principal (Mes 3).
- Riesgo detectado: dependencia externa, l√≠mites gratis, menos control.

¬øPor qu√©?
- n8n es r√°pido para prototipos, pero en producci√≥n real dependes de su pricing, estabilidad y l√≠mites.
- El PLAN busca "sistemas vivos que no mueren" y "control total".

¬øC√≥mo lo evitar√≠a la pr√≥xima?
- Priorizar siempre herramientas con control total (Python puro) antes de low-code externas.
- Evaluar dependencias externas con la pregunta: "¬øSi esta herramienta desaparece ma√±ana, mi sistema sigue vivo?"

¬øC√≥mo encaja esta semana en el plan completo?
- Mes 2 cerrado al 100%: Pandas pipeline completo (limpieza, extracci√≥n city, reporte autom√°tico).
- Pivot inteligente a Python + schedule/cron como base (control total).
- Nicho tentativo elegido: E-commerce (Shopify inventory alerts) con evidencia real de Upwork.
- Artefacto visible: Daemon autom√°tico corriendo solo, CSV clean con city, requirements.txt actualizado.
- Decisi√≥n profesional: n8n queda como opci√≥n secundaria (solo si cliente lo pide y cobro extra).

Conclusi√≥n: El plan evoluciona a m√°s resiliencia y monetizaci√≥n real.  
¬°Sistemas vivos en acci√≥n ‚Äì control total conseguido! ‚ö°

# Weekly Post-Mortem - 19 Diciembre 2025 (Cierre Mes 2 / Inicio Mes 3)

¬øQu√© se rompi√≥ esta semana?
- Warnings de Pandas (FutureWarning chained assignment).
- Dependencia inicial planeada en n8n (riesgo de l√≠mites y control bajo).

¬øPor qu√©?
- Warnings: Uso de chained assignment (df["col"] = ...) que cambiar√° en pandas 3.0.
- n8n: R√°pido para prototipos, pero dependes de pricing externo, l√≠mites gratis y menos control total.

¬øC√≥mo lo evitar√≠a la pr√≥xima?
- Warnings: Siempre usar df.loc[:, "col"] = ... para asignaciones seguras.
- Dependencias externas: Evaluar con "si desaparece ma√±ana, ¬ømi sistema vive?" ‚Üí Priorizar Python puro.

¬øC√≥mo encaja esta semana en el plan completo?
- Mes 2 cerrado al 100%: Pandas pipeline completo (limpieza, extracci√≥n city, reporte autom√°tico, warnings eliminados).
- Pivot inteligente: De n8n a Python + schedule/cron (control total, estabilidad profesional).
- Daemon autom√°tico corriendo en background (ejecuciones programadas reales).
- Nicho tentativo: E-commerce (Shopify inventory alerts) con evidencia Upwork.
- Artefactos visibles: CSV clean con city, daemon vivo, PDF del plan generado.
- requirements.txt actualizado con schedule.

Conclusi√≥n: El plan evoluciona a m√°s resiliencia y monetizaci√≥n real.  
Sistemas vivos > herramientas externas fr√°giles.  
¬°Control total conseguido! ‚ö°

# Weekly Post-Mortem - 20 Diciembre 2025 (Avance Mes 3)

¬øQu√© se rompi√≥ esta semana?
- Warnings de Pandas (chained assignment).
- Mezcla inicial de l√≥gica y automatizaci√≥n en un solo archivo.

¬øPor qu√©?
- Warnings: Asignaci√≥n chained (df["col"] = ...) que cambiar√° en pandas 3.0.
- Mezcla: Integramos schedule directamente en api_data_fetcher.py (viola separaci√≥n de responsabilidades).

¬øC√≥mo lo evitar√≠a la pr√≥xima?
- Warnings: Siempre usar df.loc[:, "col"] = ... para asignaciones seguras.
- Arquitectura: Separar l√≥gica pura (api_data_fetcher.py) de automatizaci√≥n (automation_runner.py) desde el principio.

¬øC√≥mo encaja esta semana en el plan completo?
- Mes 2 cerrado al 100%: Limpieza Pandas, extracci√≥n city segura (json.loads), reporte autom√°tico.
- Mes 3 avanzado: Runner autom√°tico separado con schedule (cada hora), fallback resiliencia.
- Arquitectura pro: L√≥gica pura + runner separado (ejecutable manual o autom√°tico).
- Nicho tentativo confirmado: E-commerce (Shopify inventory alerts).
- Artefactos visibles: Daemon corriendo, CSV clean con city, PDF del plan generado.

Conclusi√≥n: 
- Pivot a control total (Python puro > low-code).
- C√≥digo m√°s mantenible, testeable y listo para cron real.
- Sistemas vivos en acci√≥n ‚Äì separaci√≥n limpia conseguida. ‚ö°

# Weekly Post-Mortem - 20 Diciembre 2025 (Avance Mes 3)

### üöÄ API DATA FETCHER ‚Äì ALERTAS DOCUMENTADAS

Este bloque resume las alertas implementadas en el `api_data_fetcher.py`:

```python
### 1Ô∏è‚É£ ALERTA FILAS INCOMPLETAS (Missing Data)
# Revisa las columnas cr√≠ticas: id, name, email
# Si hay NaNs, genera alerta y guarda CSV opcional
def alert_missing_data(df: pd.DataFrame):
    critical_cols = ["id", "name", "email"]
    missing_rows = df[df[critical_cols].isnull().any(axis=1)]

    if not missing_rows.empty:
        alert_msg = f"üö® ALERTA: {len(missing_rows)} filas con datos cr√≠ticos faltantes"
        print(alert_msg)
        print(missing_rows[critical_cols])
        logging.warning(alert_msg)

        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        path = f"{OUTPUT_DIR}/missing_data_{ts}.csv"
        missing_rows.to_csv(path, index=False)
        print(f"üíæ CSV de registros incompletos guardado: {path}")
    else:
        print("‚ÑπÔ∏è No se detectaron filas con datos cr√≠ticos faltantes")

### 2Ô∏è‚É£ ALERTA DUPLICADOS POR EMAIL
# Detecta registros duplicados usando la columna 'email'
# Genera alerta en consola y log
duplicated_rows = df[df.duplicated(subset=["email"])]
if not duplicated_rows.empty:
    alert = f"üö® ALERTA: {len(duplicated_rows)} duplicados detectados por email"
    print(alert)
    print(duplicated_rows[["email"]])
    logging.warning(alert)
else:
    print("‚ÑπÔ∏è No se detectaron duplicados")

### 3Ô∏è‚É£ ALERTA STOCK BAJO
# Revisa columnas 'stock' y 'product_id'
# Si stock <= threshold, genera alerta y guarda CSV autom√°ticamente
def alert_low_stock(df: pd.DataFrame, threshold: int = 5):
    if "stock" not in df.columns or "product_id" not in df.columns:
        print("‚ÑπÔ∏è No se detect√≥ columna 'stock' o 'product_id', alerta de stock ignorada")
        return

    low_stock = df[df["stock"] <= threshold]

    if not low_stock.empty:
        alert_msg = f"üö® ALERTA: {len(low_stock)} productos con stock <= {threshold}"
        print(alert_msg)
        print(low_stock[["product_id", "name", "stock"]])
        logging.warning(alert_msg)

        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        path = f"{OUTPUT_DIR}/low_stock_{ts}.csv"
        low_stock.to_csv(path, index=False)
        print(f"üíæ CSV de stock cr√≠tico guardado: {path}")
    else:
        print("‚ÑπÔ∏è No hay productos con stock cr√≠tico")
        
### 4Ô∏è‚É£ ALERTA VENTAS INUSUALES / SIN VENTAS
# Revisa columnas 'product_id', 'name', 'last_sold_date'
# Si un producto no se ha vendido en m√°s de X d√≠as, genera alerta y guarda CSV autom√°ticamente
def alert_unusual_sales(df: pd.DataFrame, days_threshold: int = 30):
    if "last_sold_date" not in df.columns or "product_id" not in df.columns:
        print("‚ÑπÔ∏è No se detect√≥ columna 'last_sold_date' o 'product_id', alerta de ventas ignorada")
        return

    cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=days_threshold)
    unsold_products = df[pd.to_datetime(df["last_sold_date"]) < cutoff_date]

    if not unsold_products.empty:
        alert_msg = f"üö® ALERTA: {len(unsold_products)} productos sin ventas en los √∫ltimos {days_threshold} d√≠as"
        print(alert_msg)
        print(unsold_products[["product_id", "name", "last_sold_date"]])
        logging.warning(alert_msg)

        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        path = f"{OUTPUT_DIR}/unsold_products_{ts}.csv"
        unsold_products.to_csv(path, index=False)
        print(f"üíæ CSV de productos sin ventas guardado: {path}")
    else:
        print(f"‚ÑπÔ∏è Todos los productos tienen ventas recientes (<{days_threshold} d√≠as)")
        
### üîπ Notas generales
# - Todos los CSV de alerta se guardan autom√°ticamente en la carpeta output.
# - Las alertas se muestran en consola y tambi√©n se registran en el log.
# - La limpieza final del CSV elimina duplicados y columnas pesadas para an√°lisis.

# Weekly Post-Mortem - 20 Diciembre 2025 (Avance Mes 3)

## üîπ Cron / Automizaci√≥n

- Se configur√≥ `run_api_data_fetcher.py` como job de cron.
- Prueba r√°pida: cada minuto (`* * * * *`) para validar ejecuci√≥n.
- Logs revisados con `tail -f .../logs/cron.log`.
- CSV y JSON se generan correctamente en carpeta `output/`.
- Alertas visibles en log: stock cr√≠tico, ventas inusuales, filas incompletas.
- Script ejecutable con `chmod +x`.
- Sin errores cr√≠ticos reportados; warning FutureWarning de pandas no rompe ejecuci√≥n.

## Pr√≥ximos pasos

1. Esperar confirmaci√≥n de 2-3 ejecuciones consecutivas exitosas.
2. Cambiar cron a horario definitivo (ej.: cada hora) para full-time.
3. Revisar alertas de negocio real (ventas inusuales) y ajustar umbrales si es necesario.
4. Documentar resultados de pruebas en NOTES.md.
    
Weekly Post-Mortem - 21 Diciembre 2025 (Avance Mes 3)

üöÄ API DATA FETCHER ‚Äì EJECUCI√ìN INICIAL DOCUMENTADA
Estado actual:
Pipeline ejecutado con python3 -i api_data_fetcher.py.
Backup autom√°tico del script creado: backups/api_data_fetcher_backup_TIMESTAMP.py.
Fetch API funcionando: 10 registros obtenidos y validados.
CSV y JSON raw generados en output/.
Validaci√≥n de columnas completada: ‚úÖ todas presentes.
Alertas revisadas:
Filas incompletas: ninguna detectada.
Stock bajo: no aplica (sin columna 'stock').
Duplicados: ninguno detectado.
Procesamiento de duplicados y limpieza final correcto.
CSV limpio generado: output/users_data_TIMESTAMP_clean.csv.
Ejecuci√≥n finalizada sin errores.
Checklist de verificaci√≥n previa a cada corrida:
.env actualizado con credenciales correctas (EMAIL_PASSWORD, SHOPIFY_TOKEN, etc.).
Librer√≠as instaladas: pandas, requests, python-dotenv, schedule.
Carpetas existentes:
output/
backups/
Script actualizado y versionado (backup autom√°tico activo).
Variables globales definidas (OUTPUT_DIR, LOW_STOCK_THRESHOLD, etc.).
Funciones principales listas: fetch, validate, alerts, process, save.
SMTP listo para env√≠os de correo (prueba manual o dummy).
Logs funcionando (logs/cron.log si se ejecuta en cron).
Python 3.14 confirmado.

Pr√≥ximos pasos:
Testear flujo shopify y validar paginaci√≥n completa.
Ejecutar pipeline con CSV local de prueba para debug.
Configurar schedule o cron para automatizaci√≥n peri√≥dica.
Revisar alertas de negocio real (stock bajo, ventas inusuales) y ajustar thresholds.
Documentar resultados de cada corrida en NOTES.md.
Preparar snippet de ejecuci√≥n autom√°tica para producci√≥n.

# üß† NOTES ‚Äì Webhook Automation System

## üìÖ Fecha

22‚Äë12‚Äë2025

---

## üéØ Objetivo de esta fase

Construir y validar un **servidor de webhooks funcional**, capaz de recibir datos tipo Shopify, procesarlos y generar diagn√≥sticos autom√°ticos con evidencia.

---

## ‚úÖ Logros confirmados

* Flask server levantado correctamente en `:5001`
* Endpoint `/webhook/shopify` operativo
* Payload recibido y visible en terminal
* Conversi√≥n correcta a `DataFrame`
* Alertas ejecutadas:

  * Stock bajo
  * Sin ventas
  * Datos faltantes
* CSVs generados autom√°ticamente en `/output`

---

## üß© Problemas resueltos (importante)

### 1. Imports rotos

* Causa: m√≥dulos no existentes o sin `__init__.py`
* Soluci√≥n:

  * Crear estructura correcta
  * Ejecutar `setup_project.py`

### 2. Error `logging not defined`

* Causa: uso de `logging` sin import
* Soluci√≥n:

```python
import logging
```

### 3. Confusi√≥n de puertos

* Flask corre en **5001**, no 5000
* Curl debe apuntar al puerto correcto

---

## üèóÔ∏è Arquitectura validada

* Flask = capa de entrada
* fetchers = ingesti√≥n
* alerts = reglas de negocio
* diagnostics = limpieza / outputs
* output = evidencia (no logs ocultos)

Esto cumple est√°ndar **MVP vendible**.

---

## üß† Decisiones t√©cnicas clave

* CSV como output principal (auditable)
* Sin DB por ahora (simplicidad > complejidad)
* C√≥digo modular (escala f√°cil)
* Errores visibles, no silenciosos

---

## üöÄ Pr√≥ximos bloques (confirmados)

### 2Ô∏è‚É£ Conectar Shopify real

* API REST
* Token privado
* Webhook real desde admin Shopify

### 3Ô∏è‚É£ Automatizar

* `cron` (producci√≥n)
* `schedule` (local / demo)

---

## üß± Estado mental del proyecto

> Esto **ya no es pr√°ctica**, es un sistema real.

Base s√≥lida para:

* Portafolio
* Side income
* Cliente real

Seguimos.


---

## 3Ô∏è‚É£ Contenido sugerido para `NOTES.md`

```markdown
# NOTES.md - Python Automation

## √öltimas pruebas
- [x] Webhook Shopify simulado correctamente con ngrok.
- [x] CSV de alertas generados (`low_stock`, `simulation_test`).
- [x] Variables de entorno cargadas desde `.env`.
- [x] Configuraci√≥n segura para GitHub (secrets ignorados).

## Pr√≥ximos pasos
1. Validar HMAC de Shopify en `webhook_server.py`.
2. Organizar `config.py` central para todas las variables.
3. Automatizaci√≥n programada con `schedule` / cron.
4. Documentar funciones clave en cada m√≥dulo.

## Observaciones
- Mantener `.env` y `security.env` fuera de GitHub.
- Archivos generados y logs solo locales.
- Subir √∫nicamente scripts y documentaci√≥n.
