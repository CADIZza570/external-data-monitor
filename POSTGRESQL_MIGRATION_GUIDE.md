# üêò GU√çA DE MIGRACI√ìN A POSTGRESQL
## Cerebro Central v2.0 ‚Üí PostgreSQL

**√öltima actualizaci√≥n:** 2026-02-02
**Autor:** Claude (Cirujano Maestro)
**Status:** PREPARADO PARA MIGRACI√ìN

---

## üìã √çNDICE

1. [Variables que deben cambiar](#1-variables-que-deben-cambiar)
2. [M√≥dulos afectados](#2-m√≥dulos-afectados)
3. [Cambios en c√≥digo](#3-cambios-en-c√≥digo)
4. [Schema PostgreSQL](#4-schema-postgresql)
5. [Plan de migraci√≥n](#5-plan-de-migraci√≥n)
6. [Rollback strategy](#6-rollback-strategy)
7. [Testing checklist](#7-testing-checklist)

---

## 1. VARIABLES QUE DEBEN CAMBIAR

### 1.1 Variables de Entorno (Railway)

**NUEVAS (agregar):**
```bash
# PostgreSQL Connection
DATABASE_URL=postgresql://user:password@host:5432/dbname
POSTGRES_HOST=hostname.railway.app
POSTGRES_PORT=5432
POSTGRES_DB=tiburon_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=***************

# Connection Pool
DB_POOL_MIN_SIZE=5
DB_POOL_MAX_SIZE=20
DB_POOL_TIMEOUT=30
```

**OBSOLETAS (eliminar despu√©s de migraci√≥n):**
```bash
# Ya no se usar√°n con PostgreSQL
SQLITE_DB_PATH=/data/webhooks.db
```

**MANTENER:**
```bash
# Estas NO cambian
SHOPIFY_WEBHOOK_SECRET=***
MAKE_WEBHOOK_URL=https://hook.us2.make.com/***
ADMIN_API_KEY=shark-predator-2026
SENDGRID_API_KEY=***
DISCORD_WEBHOOK_URL=***
```

---

### 1.2 Constantes en C√≥digo

**`cerebro_central.py`**
- ‚úÖ **NO REQUIERE CAMBIOS** - Ya est√° modularizado
- Usa `get_db_connection()` de `database.py`
- Todas las queries SQL son compatibles PostgreSQL

**`metrics_calculator.py`**
- ‚úÖ **NO REQUIERE CAMBIOS** - Sin dependencias de DB
- Totalmente agn√≥stico a la base de datos
- Recibe/retorna dicts gen√©ricos

**`database.py` - CR√çTICO**
- ‚ö†Ô∏è **REQUIERE REFACTORIZACI√ìN COMPLETA**
- Cambiar de `sqlite3` a `psycopg2` o `asyncpg`
- Ver secci√≥n 3.1

---

## 2. M√ìDULOS AFECTADOS

### 2.1 Alta Prioridad (Cr√≠tico para funcionar)

| Archivo | Impacto | Cambios Requeridos |
|---------|---------|-------------------|
| `database.py` | üî¥ CR√çTICO | Refactorizar conexi√≥n SQLite ‚Üí PostgreSQL |
| `webhook_server.py` | üü° MEDIO | Actualizar imports si cambia estructura de database.py |
| `cerebro_central.py` | üü¢ BAJO | Sin cambios si database.py mantiene misma interfaz |
| `metrics_calculator.py` | üü¢ NINGUNO | M√≥dulo agn√≥stico - sin cambios |

### 2.2 Baja Prioridad (Funciones legacy)

| Archivo | Impacto | Acci√≥n |
|---------|---------|--------|
| `cashflow_api.py` | üü° MEDIO | Revisar queries SQLite espec√≠ficas |
| `migrate_db_cashflow.py` | üî¥ OBSOLETO | Eliminar despu√©s de migraci√≥n |
| `productos_inventario_completo.py` | üü° BAJO | Actualizar si usa conexi√≥n directa |
| `*.db` files | üî¥ OBSOLETO | Respaldar y archivar |

---

## 3. CAMBIOS EN C√ìDIGO

### 3.1 `database.py` - Refactorizaci√≥n Completa

**ANTES (SQLite):**
```python
import sqlite3

def get_db_connection():
    """Conexi√≥n SQLite"""
    conn = sqlite3.connect('/data/webhooks.db', timeout=10)
    conn.row_factory = sqlite3.Row
    return conn
```

**DESPU√âS (PostgreSQL):**
```python
import psycopg2
import psycopg2.extras
import os

def get_db_connection():
    """Conexi√≥n PostgreSQL con pool"""
    conn = psycopg2.connect(
        host=os.getenv('POSTGRES_HOST'),
        port=os.getenv('POSTGRES_PORT', 5432),
        database=os.getenv('POSTGRES_DB'),
        user=os.getenv('POSTGRES_USER'),
        password=os.getenv('POSTGRES_PASSWORD'),
        cursor_factory=psycopg2.extras.RealDictCursor  # Retorna dicts como SQLite Row
    )
    return conn
```

**CAMBIOS CLAVE:**
1. ‚úÖ `RealDictCursor` hace que rows sean dicts (compatible con c√≥digo existente)
2. ‚úÖ Variables de entorno en lugar de archivo local
3. ‚úÖ Sin cambios en interfaz - `cerebro_central.py` sigue funcionando igual

---

### 3.2 Queries SQL - Compatibilidad

**REVISAR ESTAS DIFERENCIAS:**

| Feature | SQLite | PostgreSQL |
|---------|--------|------------|
| Auto-increment | `AUTOINCREMENT` | `SERIAL` o `BIGSERIAL` |
| Timestamp | `TIMESTAMP DEFAULT CURRENT_TIMESTAMP` | `TIMESTAMP DEFAULT NOW()` |
| Upsert | `INSERT ... ON CONFLICT ... DO UPDATE` | ‚úÖ COMPATIBLE |
| JSON | `TEXT` (serializado) | `JSON` o `JSONB` (nativo) ‚úÖ MEJOR |
| Full-text search | FTS5 (limitado) | `tsvector` ‚úÖ M√ÅS POTENTE |

**QUERIES EN `cerebro_central.py` - STATUS:**
- ‚úÖ `SELECT * FROM products WHERE sku = ?` ‚Üí Compatible (cambiar `?` a `%s`)
- ‚úÖ `UPDATE products SET stock = ? WHERE sku = ?` ‚Üí Compatible
- ‚úÖ `INSERT ... ON CONFLICT DO UPDATE` ‚Üí Compatible
- ‚úÖ Todas las queries son ANSI SQL est√°ndar

---

### 3.3 Placeholders - CR√çTICO

**SQLite usa `?`, PostgreSQL usa `%s`:**

```python
# ANTES (SQLite)
conn.execute('SELECT * FROM products WHERE sku = ?', (sku,))

# DESPU√âS (PostgreSQL)
cursor.execute('SELECT * FROM products WHERE sku = %s', (sku,))
```

**SOLUCI√ìN CENTRALIZADA:**
Crear helper en `database.py`:
```python
def execute_query(conn, query, params=None):
    """
    Ejecuta query compatible con PostgreSQL.

    Maneja autom√°ticamente placeholders.
    """
    # PostgreSQL usa %s en lugar de ?
    query_pg = query.replace('?', '%s')

    cursor = conn.cursor()
    cursor.execute(query_pg, params or ())
    return cursor
```

---

## 4. SCHEMA POSTGRESQL

### 4.1 Tabla `products` (optimizada)

```sql
CREATE TABLE products (
    -- Primary Key
    id BIGSERIAL PRIMARY KEY,

    -- Identifiers
    product_id VARCHAR(255) NOT NULL,
    sku VARCHAR(255) UNIQUE NOT NULL,
    name VARCHAR(500) NOT NULL,

    -- Inventory
    stock INTEGER DEFAULT 0,
    cost_price DECIMAL(10, 2),
    price DECIMAL(10, 2),

    -- Metrics (calculadas por MetricsCalculator)
    velocity_daily DECIMAL(10, 4) DEFAULT 0,
    total_sales_30d INTEGER DEFAULT 0,
    total_sales_60d INTEGER DEFAULT 0,

    -- Timestamps
    last_sale_date TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),

    -- √çndices
    INDEX idx_sku (sku),
    INDEX idx_product_id (product_id),
    INDEX idx_velocity (velocity_daily DESC),
    INDEX idx_stock_low (stock) WHERE stock < 10
);
```

**MEJORAS vs SQLite:**
1. ‚úÖ `BIGSERIAL` en lugar de `INTEGER AUTOINCREMENT`
2. ‚úÖ `DECIMAL` para precios (precisi√≥n exacta vs REAL)
3. ‚úÖ √çndices parciales (`WHERE stock < 10`) - m√°s eficientes
4. ‚úÖ `updated_at` con trigger autom√°tico

---

### 4.2 Tabla `daily_sales` (optimizada)

```sql
CREATE TABLE daily_sales (
    date DATE PRIMARY KEY,
    total_sales DECIMAL(12, 2) DEFAULT 0,
    orders_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW(),

    -- √çndice para queries de rango
    INDEX idx_date_range (date DESC)
);
```

---

### 4.3 Tabla `webhooks` (audit log)

```sql
CREATE TABLE webhooks (
    id BIGSERIAL PRIMARY KEY,
    webhook_type VARCHAR(100),
    order_id VARCHAR(255),
    order_number VARCHAR(255),
    payload JSONB,  -- ‚úÖ JSON nativo en lugar de TEXT
    received_at TIMESTAMP DEFAULT NOW(),
    processed BOOLEAN DEFAULT TRUE,

    -- √çndices para b√∫squeda
    INDEX idx_order_id (order_id),
    INDEX idx_received_at (received_at DESC),
    INDEX idx_payload_gin (payload) USING GIN  -- ‚úÖ B√∫squeda full-text en JSON
);
```

**VENTAJA JSONB:**
- B√∫squedas dentro del JSON: `payload->>'customer'->>'email' = 'test@example.com'`
- √çndices GIN para queries r√°pidas
- Validaci√≥n autom√°tica de estructura JSON

---

## 5. PLAN DE MIGRACI√ìN

### 5.1 Fase 1: Preparaci√≥n (1 d√≠a)

**Acciones:**
1. ‚úÖ Crear nueva instancia PostgreSQL en Railway
2. ‚úÖ Ejecutar schema SQL (secci√≥n 4)
3. ‚úÖ Configurar variables de entorno en Railway
4. ‚úÖ Backup completo de SQLite actual

**Comando backup:**
```bash
# Desde Railway CLI
railway run sqlite3 /data/webhooks.db ".backup /data/backup_$(date +%Y%m%d).db"
railway volume download /data/backup_*.db
```

---

### 5.2 Fase 2: Migraci√≥n de Datos (2-4 horas)

**Script de migraci√≥n:**
```python
# migrate_sqlite_to_postgres.py
import sqlite3
import psycopg2
import psycopg2.extras
import os

def migrate_products():
    """Migrar tabla products de SQLite a PostgreSQL."""

    # Conectar a ambas DBs
    sqlite_conn = sqlite3.connect('/data/webhooks.db')
    sqlite_conn.row_factory = sqlite3.Row

    pg_conn = psycopg2.connect(os.getenv('DATABASE_URL'))

    # Leer todos los productos
    sqlite_rows = sqlite_conn.execute('SELECT * FROM products').fetchall()

    # Insertar en PostgreSQL
    pg_cursor = pg_conn.cursor()

    for row in sqlite_rows:
        pg_cursor.execute('''
            INSERT INTO products (
                product_id, sku, name, stock, cost_price, price,
                velocity_daily, total_sales_30d, last_sale_date,
                created_at, updated_at
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
            ON CONFLICT (sku) DO UPDATE SET
                stock = EXCLUDED.stock,
                velocity_daily = EXCLUDED.velocity_daily,
                total_sales_30d = EXCLUDED.total_sales_30d
        ''', (
            row['product_id'], row['sku'], row['name'],
            row['stock'], row['cost_price'], row['price'],
            row['velocity_daily'], row['total_sales_30d'],
            row['last_sale_date']
        ))

    pg_conn.commit()
    print(f"‚úÖ Migrados {len(sqlite_rows)} productos")

def migrate_daily_sales():
    """Migrar daily_sales."""
    # Similar a migrate_products()
    pass

def migrate_webhooks():
    """Migrar webhooks (√∫ltimos 30 d√≠as solo)."""
    # Solo migrar √∫ltimos 30 d√≠as para reducir carga
    pass

if __name__ == '__main__':
    print("üöÄ Iniciando migraci√≥n SQLite ‚Üí PostgreSQL")
    migrate_products()
    migrate_daily_sales()
    migrate_webhooks()
    print("‚úÖ Migraci√≥n completada")
```

**Ejecutar:**
```bash
railway run python migrate_sqlite_to_postgres.py
```

---

### 5.3 Fase 3: Refactorizaci√≥n de C√≥digo (4 horas)

**Orden de cambios:**

1. **`database.py`** (1 hora)
   - Refactorizar `get_db_connection()`
   - Crear helpers para queries compatibles
   - Testing de conexi√≥n

2. **Actualizar placeholders `?` ‚Üí `%s`** (1 hora)
   - Buscar todos los `execute()` en el c√≥digo
   - Reemplazar placeholders
   - Testing de cada query

3. **`requirements.txt`** (5 min)
   ```
   # Agregar
   psycopg2-binary==2.9.9

   # Remover despu√©s de migraci√≥n
   # sqlite3 (built-in, no package)
   ```

4. **Testing local** (2 horas)
   - Ver secci√≥n 7

---

### 5.4 Fase 4: Deploy a Railway (30 min)

**Checklist:**
```bash
# 1. Commit cambios
git add .
git commit -m "Feat: Migraci√≥n SQLite ‚Üí PostgreSQL (#18)"

# 2. Push a Railway
git push origin main

# 3. Verificar logs
railway logs

# 4. Healthcheck
curl https://tranquil-freedom-production.up.railway.app/health

# 5. Test webhook
curl -X POST -H "X-Admin-Key: shark-predator-2026" \
  -H "Content-Type: application/json" \
  -d '{"id": 9999, "order_number": 9999, "total_price": "100", "customer": {"first_name": "Test"}, "line_items": [{"sku": "TEST", "title": "Test", "quantity": 1, "price": "100"}]}' \
  https://tranquil-freedom-production.up.railway.app/api/webhook/shopify/orders
```

---

## 6. ROLLBACK STRATEGY

### 6.1 Si falla la migraci√≥n

**Opci√≥n 1: Revertir c√≥digo (5 min)**
```bash
# Volver a commit anterior
git revert HEAD
git push origin main
```

**Opci√≥n 2: Cambiar a branch anterior (2 min)**
```bash
# Crear branch de backup antes de migrar
git checkout -b backup-sqlite
git push origin backup-sqlite

# Si falla, volver
git checkout backup-sqlite
railway up
```

### 6.2 Mantener SQLite como fallback (RECOMENDADO)

**En `database.py`:**
```python
def get_db_connection():
    """Conexi√≥n con fallback SQLite."""
    use_postgres = os.getenv('USE_POSTGRES', 'true').lower() == 'true'

    if use_postgres:
        try:
            return _get_postgres_connection()
        except Exception as e:
            logger.error(f"PostgreSQL fall√≥: {e}, usando SQLite fallback")
            return _get_sqlite_connection()
    else:
        return _get_sqlite_connection()
```

**Variable Railway:**
```bash
# Si PostgreSQL falla, cambiar a:
USE_POSTGRES=false

# Sistema vuelve a SQLite autom√°ticamente
```

---

## 7. TESTING CHECKLIST

### 7.1 Pre-Migraci√≥n (SQLite actual)

- [ ] Backup completo de webhooks.db
- [ ] Exportar datos cr√≠ticos a CSV
- [ ] Documentar queries custom en otros archivos
- [ ] Verificar que Cerebro Central v2.0 funciona

### 7.2 Post-Migraci√≥n (PostgreSQL)

**Testing funcional:**
- [ ] Conexi√≥n a PostgreSQL exitosa
- [ ] Tabla `products` con datos migrados
- [ ] Tabla `daily_sales` con datos migrados
- [ ] Webhook simulado funciona
- [ ] Metrics calculator funciona (ROI, velocity, coverage)
- [ ] Alertas se generan correctamente
- [ ] Make.com recibe JSON correcto
- [ ] WhatsApp muestra mensaje con tallas

**Testing de performance:**
- [ ] Query `SELECT * FROM products WHERE sku = %s` < 10ms
- [ ] Insert webhook < 50ms
- [ ] Update metrics < 20ms
- [ ] Webhook completo end-to-end < 500ms

**Testing de carga:**
```bash
# 100 webhooks concurrentes
ab -n 100 -c 10 -p test_payload.json -T application/json \
  -H "X-Admin-Key: shark-predator-2026" \
  https://tranquil-freedom-production.up.railway.app/api/webhook/shopify/orders
```

---

## 8. DICCIONARIOS MANUALES ‚Üí TABLAS

### 8.1 Actualmente en C√≥digo (SQLite era)

**NO HAY diccionarios manuales cr√≠ticos en Cerebro Central v2.0** ‚úÖ

Todos los datos ya est√°n en tablas:
- `products` - Productos y m√©tricas
- `daily_sales` - Ventas diarias
- `webhooks` - Audit log

### 8.2 Si encuentras diccionarios en archivos legacy

**ANTES (hardcoded):**
```python
CATEGORIES = {
    'BTA-CG': 'Botas Casuales',
    'BTA-PTN': 'Botas Patentadas',
    'ZPT': 'Zapatos'
}
```

**DESPU√âS (tabla PostgreSQL):**
```sql
CREATE TABLE product_categories (
    code VARCHAR(50) PRIMARY KEY,
    name VARCHAR(200) NOT NULL
);

-- Migrar dict
INSERT INTO product_categories VALUES ('BTA-CG', 'Botas Casuales');
```

**En c√≥digo:**
```python
def get_category_name(code):
    conn = get_db_connection()
    result = conn.execute('SELECT name FROM product_categories WHERE code = %s', (code,))
    return result.fetchone()['name'] if result else 'Unknown'
```

---

## 9. MEJORAS ADICIONALES POST-MIGRACI√ìN

### 9.1 Connection Pooling (Recomendado)

```python
# En database.py
from psycopg2 import pool

# Pool global
connection_pool = None

def init_connection_pool():
    """Inicializa pool de conexiones."""
    global connection_pool
    connection_pool = pool.SimpleConnectionPool(
        minconn=5,
        maxconn=20,
        host=os.getenv('POSTGRES_HOST'),
        port=os.getenv('POSTGRES_PORT'),
        database=os.getenv('POSTGRES_DB'),
        user=os.getenv('POSTGRES_USER'),
        password=os.getenv('POSTGRES_PASSWORD')
    )

def get_db_connection():
    """Obtiene conexi√≥n del pool."""
    return connection_pool.getconn()

def return_connection(conn):
    """Devuelve conexi√≥n al pool."""
    connection_pool.putconn(conn)
```

**Beneficios:**
- ‚úÖ Reduce latencia (no crear conexi√≥n cada vez)
- ‚úÖ Maneja concurrencia mejor
- ‚úÖ Previene "too many connections"

---

### 9.2 Migraciones Versionadas (Alembic)

```bash
pip install alembic

# Inicializar
alembic init alembic

# Crear migraci√≥n
alembic revision -m "Add products table"

# Ejecutar migraci√≥n
alembic upgrade head
```

**Beneficio:** Versionado de schema como c√≥digo

---

### 9.3 Read Replicas (Escalabilidad futura)

```python
# Conexi√≥n master (writes)
POSTGRES_MASTER_URL = os.getenv('DATABASE_URL')

# Conexi√≥n replica (reads)
POSTGRES_REPLICA_URL = os.getenv('DATABASE_REPLICA_URL')

def get_db_connection(readonly=False):
    """
    Obtiene conexi√≥n seg√∫n tipo de operaci√≥n.

    Args:
        readonly: Si True, usa replica (queries SELECT)
    """
    url = POSTGRES_REPLICA_URL if readonly else POSTGRES_MASTER_URL
    return psycopg2.connect(url)
```

---

## 10. CONTACTOS Y RECURSOS

**Railway PostgreSQL:**
- Docs: https://docs.railway.app/databases/postgresql
- Pricing: $5/month base + usage

**psycopg2 Docs:**
- https://www.psycopg.org/docs/

**SQL Compatibility:**
- https://wiki.postgresql.org/wiki/SQLite_to_PostgreSQL

---

## ‚úÖ CHECKLIST FINAL PRE-MIGRACI√ìN

**Antes de ejecutar la migraci√≥n, verificar:**

- [ ] C√≥digo v2.0 funciona perfectamente en SQLite
- [ ] Backup de webhooks.db descargado
- [ ] PostgreSQL instance creada en Railway
- [ ] Variables de entorno configuradas
- [ ] Script de migraci√≥n probado localmente
- [ ] Plan de rollback documentado
- [ ] Equipo notificado del mantenimiento
- [ ] Ventana de downtime acordada (estimar 1-2 horas)

---

**ü¶à TIBUR√ìN LISTO PARA SALTAR A POSTGRESQL** üêò

